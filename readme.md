# **# ðŸ•·ï¸ Froxy**

> A chill, open-source web engine that crawls, indexes, and vibes with web content.

![froxy banner](https://github.com/MultiX0/froxy/blob/main/banner.png?raw=true)


---

## ðŸ’¡ What is Froxy?

Froxy is a modular full-stack web engine designed to crawl web pages, extract content, index it using TF-IDF, and make it searchable â€” all powered by modern tools. It includes:

* A **Go**-based crawler (aka the spider ðŸ•·ï¸)
* A **Node.js** indexer & search engine
* A **PostgreSQL** database
* A **Next.js** front-end UI (fully integrated with real APIs)

This project is built for learning, experimenting, and extending â€” great for developers who want to understand how search engines work from scratch.

> Fun fact: I made this project in just **3 days** â€” so it might not be perfect, but you know what?
> **It works!**
>
> *(Weâ€™ll keep evolving this codebase together â¤ï¸)*

---

## ðŸ” Features

* ðŸŒ Crawl websites (Go)
* ðŸ¤” Index & search content (TF-IDF, Node.js)
* ðŸ•º Store in PostgreSQL
* ðŸŽ¨ Modern UI in Next.js + Tailwind

> The frontend is fully connected to the backend and only uses mock data for suggestions.

---

## ðŸ“‚ Folder Structure

```
froxy/
â”œâ”€â”€ front-end/          # Next.js frontend
â”‚   â”œâ”€â”€ app/            # App routes (search, terms, about, etc.)
â”‚   â”œâ”€â”€ components/     # UI components (shadcn-style)
â”‚   â”œâ”€â”€ hooks/          # React hooks
â”‚   â”œâ”€â”€ lib/            # Utility logic
â”‚   â”œâ”€â”€ public/         # Static assets
â”‚   â””â”€â”€ styles/         # TailwindCSS setup
â”œâ”€â”€ indexer-search/     # Node.js indexer/search backend
â”‚   â””â”€â”€ lib/
â”‚       â”œâ”€â”€ functions/  # TF-IDF, parser
â”‚       â”œâ”€â”€ services/   # DB + search service
â”‚       â””â”€â”€ utils/      # Helper utilities
â”œâ”€â”€ spider/             # Web crawler in Go
â”‚   â”œâ”€â”€ db/             # DB handling
â”‚   â”œâ”€â”€ functions/      # Crawl logic
â”‚   â”œâ”€â”€ models/         # Data models
â”‚   â””â”€â”€ utils/          # Misc helpers
â”œâ”€â”€ db/                 # PostgreSQL schema & backup scripts
â”‚   â””â”€â”€ scripts/        # Shell backups
â”œâ”€â”€ froxy.sh            # Setup & runner script
â”œâ”€â”€ LICENSE             # MIT License
â””â”€â”€ readme.md           # This file
```

---

## âš™ï¸ Getting Started

### Requirements

* Node.js (18+)
* pnpm or npm
* Go (1.18+)
* Docker
* PostgreSQL instance

### Run it locally

```bash
# 1. Start the database
cd db
docker-compose up -d

# 2. Run the crawler first (this collects website content from the URLs you provide)
cd ../spider
go run main.go

# 3. Once you've crawled enough, start the indexer
cd ../indexer-search
npm install
npm start

# 4. Launch the front-end
cd ../front-end
npm i --legacy-peer-deps
npm run dev
```

ðŸ’¡ **Pro tip**: If you donâ€™t want to mess with each service manually, just run:

```bash
./froxy.sh
```

This script handles crawling and indexing in one go, so you can just focus on exploring the results.

---

## ðŸ” Environment Variables

Each folder has its own `.env` file. Here's what you need to set up:

### For `indexer-search/`

```env
DB_HOST=localhost
DB_PORT=5432
DB_USER=your_user
DB_PASSWORD=your_password
DB_NAME=your_database
DB_SSLMODE=disable  # or prefer, require, etc.
API_KEY=your_api_key
PORT=8080
```

### For `front-end/`

```env
API_URL=http://localhost:8080
API_KEY=your_api_key
```

### For `db/`

```env
POSTGRES_DB=your_database
POSTGRES_USER=your_user
POSTGRES_PASSWORD=your_password
DB_NAME=your_database
DB_SSLMODE=require # or prefer, require, etc.
```

### For `spider/`

```env
DB_HOST=localhost
DB_PORT=5432
DB_USER=your_user
DB_PASSWORD=your_password
DB_NAME=your_database
DB_SSLMODE=disable # or prefer, require, etc.
```

> ðŸ’¡ You can use `DB_SSLMODE=disable` if you donâ€™t want to use SSL.

---

## ðŸ¤” How it works

1. **Crawler** pulls website content and metadata from your provided URLs
2. **Indexer** parses and weights using TF-IDF
3. **PostgreSQL** stores the indexed records
4. **Frontend** serves the search UI + suggestions

To customize crawling sources, edit this section in `spider/main.go`:

```go
var crawlableSites = []string{
    "https://en.wikipedia.org/wiki/Main_Page",
}
```

Or just run `./froxy.sh` to keep things simple.

---

## ðŸ“™ Tech Stack

* ðŸ•·ï¸ Go (Golang) â€“ crawler
* ðŸ’ª Node.js â€“ indexer, TF-IDF
* ðŸ“€ PostgreSQL â€“ database
* âš›ï¸ Next.js â€“ frontend
* ðŸŽ¨ TailwindCSS + shadcn/ui â€“ UI components
* ðŸ³ Docker â€“ database container

---

## ðŸ“¬ Want to contribute?

* Fork it ðŸŒ›
* Open a PR ðŸš°
* Share your ideas ðŸ’¡

---

## ðŸ“œ License

**MIT** â€” feel free to fork, remix, and learn from it.

---

Made with â¤ï¸ for the curious minds of the internet.

Stay weird. Stay building.

> "Not all who wander are lost â€” some are just crawling the web."
